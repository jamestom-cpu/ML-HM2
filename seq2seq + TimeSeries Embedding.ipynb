{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddf0fab",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports and Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4d02c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "import scipy.io as sio\n",
    "import joblib\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d33890",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5290610",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3509d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vars\\\\scalers\\\\min_max_scaler.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Training.csv')\n",
    "\n",
    "# split into test and training datasets\n",
    "test_size = int(dataset.shape[0]//10) # size of the test set is 10% of the training\n",
    "\n",
    "X_train_raw = dataset.iloc[:-test_size]\n",
    "X_test_raw = dataset.iloc[-test_size:]\n",
    "\n",
    "#normalize the data\n",
    "\n",
    "# choice of scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fitting the scaler\n",
    "scaler.fit(X_train_raw)\n",
    "\n",
    "#transform X_train_raw and X_test_raw\n",
    "X_pp_train = scaler.transform(X_train_raw)\n",
    "X_pp_test = scaler.transform(X_test_raw)\n",
    "\n",
    "# scaler returns numpy arrays (type = ndarray). Reconvert to dataframe for better plotting\n",
    "data = np.concatenate((X_pp_train,X_pp_test))\n",
    "data = pd.DataFrame(data, columns = dataset.columns)\n",
    "X_pp_train = data.iloc[:-test_size]\n",
    "X_pp_test = data.iloc[-test_size:]\n",
    "\n",
    "# save the scikit learn scaler \n",
    "scaler_dir = 'vars\\\\scalers'\n",
    "if not os.path.exists(scaler_dir):\n",
    "    os.makedirs(scaler_dir)\n",
    "\n",
    "scaler_path = os.path.join(scaler_dir, 'min_max_scaler.gz')\n",
    "joblib.dump(scaler, scaler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a02ea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645d5518",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First objective should be to predict the next quarter of test length. You can look at the instructions for the competition for \n",
    "# more information. So the total length to predict is c_telescope = 1152. In the first part of the competition we must predict 3 \n",
    "# quarters of the total test sequences, i.e. 3/4 * 1152 = 864. One single quarter is made up of 1152/4 = 288. I believe, whatever\n",
    "# the final structure of the model, that we could firs predict a quarter and then recgressively predict the rest.\n",
    "\n",
    "quarter_tel = (1152//4) #288\n",
    "\n",
    "# We can try to predict the quarter in 4 steps of 36\n",
    "\n",
    "reg = 4\n",
    "telescope = 80\n",
    "assert quarter_tel%4 == 0\n",
    "\n",
    "# target labels are all the labels\n",
    "target_labels = dataset.columns\n",
    "\n",
    "# Hyper parameters for how to build the sequences. The total length of the dataset is 68528, so quite large. I would take the\n",
    "# usual length to start with. We will obtain a very large number of sequences. Note that in \n",
    "# the inspection of the data the beginning of the sequence seemed quite different from the later evolution. One could consider removing \n",
    "# the first part as a possibility. \n",
    "\n",
    "window = 300\n",
    "stride = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37936793",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# build sequence function\n",
    "def build_sequences(df, target_labels=['pollution'], window=200, stride = 20, telescope=100):\n",
    "    # check to avoid errrors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = df.copy().values #copy() makes a deep copy\n",
    "    temp_labels = df[target_labels].copy().values\n",
    "    need_4_padding = len(df)%window != 0\n",
    "    \n",
    "    if(need_4_padding):\n",
    "        padding_len = window-len(df)%window\n",
    "        padding = np.zeros((padding_len, temp_df.shape[1]), dtype='float64')\n",
    "        temp_df = np.concatenate((padding, df))\n",
    "        padding = np.zeros((padding_len, temp_labels.shape[1]), dtype='float64')\n",
    "        temp_labels = np.concatenate((padding, temp_labels))\n",
    "        assert len(temp_df)%window == 0\n",
    "        \n",
    "    for idx in np.arange(0, len(temp_df)-window-telescope, stride):\n",
    "        dataset.append(temp_df[idx:idx+window])\n",
    "        labels.append(temp_labels[idx+window:idx+window+telescope])\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df436be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = build_sequences(\n",
    "    X_pp_train,\n",
    "    target_labels=target_labels,\n",
    "    window=window,\n",
    "    stride=stride,\n",
    "    telescope=telescope\n",
    "    )\n",
    "X_test, y_test = build_sequences(\n",
    "    X_pp_test,\n",
    "    target_labels=target_labels,\n",
    "    window=window,\n",
    "    stride=stride,\n",
    "    telescope=telescope\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b00a5",
   "metadata": {},
   "source": [
    "## Tokenize the TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c7a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"tokenize\" the sequences\n",
    "token_length = 20\n",
    "\n",
    "def tokenize(X, token_length=20, step = 20):\n",
    "    assert window%token_length == 0\n",
    "    return X.reshape(X.shape[0], -1, token_length, X_train.shape[2])\n",
    "\n",
    "\n",
    "# Xtokens = tokenize(X_train, token_length)\n",
    "# ytokens = tokenize(y_train, token_length)\n",
    "# print(Xtokens.shape)\n",
    "# print(ytokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50cf30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tokenization\n",
    "\n",
    "# plotting function \n",
    "def inspect_tokenization(X, tokens, columns, idx=None):\n",
    "    if(idx==None):\n",
    "        idx=np.random.randint(0,len(X))\n",
    "    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n",
    "    for i, col in enumerate(columns):\n",
    "        for j in range(tokens.shape[1]):\n",
    "            _token = tokens[idx, j, ...]\n",
    "            axs[i].plot(np.arange(j*token_length, (j+1)*token_length), _token[:, i], color='orange')\n",
    "        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n",
    "        axs[i].set_title(col)\n",
    "        axs[i].set_ylim(0,1)\n",
    "        # print(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+1))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826e76b",
   "metadata": {},
   "source": [
    "## Embed and Deembed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b080e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_6 (Bidirectio  (None, 20, 128)          36864     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 20, 64)           41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 32)               10368     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,448\n",
      "Trainable params: 88,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 32)]              0         \n",
      "                                                                 \n",
      " latent_representation (Repe  (None, 20, 32)           0         \n",
      " atVector)                                                       \n",
      "                                                                 \n",
      " decoder (Sequential)        (None, 20, 7)             89863     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,863\n",
      "Trainable params: 89,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models_dir = 'models'\n",
    "autoencoder = tfk.models.load_model(os.path.join(models_dir, 'autoencoder_w20B'))\n",
    "\n",
    "encoder = autoencoder.get_layer('Encoder')\n",
    "encoder.summary()\n",
    "\n",
    "decoder_input = tfkl.Input(shape=(32), name='decoder_input')\n",
    "repeatlayer = autoencoder.get_layer('latent_representation')(decoder_input)\n",
    "d = autoencoder.get_layer('decoder')\n",
    "decoder = tfk.Model(inputs=decoder_input, outputs=d(repeatlayer), name='Decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c669c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple embed and deembed functions\n",
    "\n",
    "def embed(tokens):\n",
    "    t = tokens.reshape(-1, tokens.shape[-2], tokens.shape[-1])\n",
    "    embedded = encoder.predict(t)\n",
    "    embedded = embedded.reshape(tokens.shape[0], tokens.shape[1], -1)\n",
    "    return embedded\n",
    "        \n",
    "def deembed(e):\n",
    "    t = e.reshape(-1, e.shape[-1])\n",
    "    d = decoder.predict(t)\n",
    "    d = d.reshape(e.shape[0], e.shape[1], token_length, 7)\n",
    "    return d        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3b4d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify the embedding and inspect the embedding/deembedding functions\n",
    "# inspect_tokenization(X_train, deembed(embed(tokenize(X_train))), dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed7e7d",
   "metadata": {},
   "source": [
    "### Embed the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe08b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6142, 15, 32), (6142, 4, 32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_emb = embed(tokenize(X_train))\n",
    "y_emb = embed(tokenize(y_train))\n",
    "\n",
    "X_emb.shape, y_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31856f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d54fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd396a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a7edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39e71d3f",
   "metadata": {},
   "source": [
    "## Build the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a6ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "\n",
    "# model hyperparams\n",
    "input_shape = X_emb.shape[1:]\n",
    "output_shape = y_emb.shape[1:]\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f171212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model \n",
    "# Train with teacher forcing\n",
    "\n",
    "def sequence2sequence(\n",
    "    embedded_dims, input_shape, telescope\n",
    "):\n",
    "    # Build Encoder-Decoder Model\n",
    "    # ---------------------------\n",
    "    \n",
    "    # ENCODER\n",
    "    # -------\n",
    "    encoder_input = tf.keras.Input(\n",
    "        shape=input_shape, name = 'encoder_input'\n",
    "    )\n",
    "    lstm1 = tfkl.LSTM(128, activation='relu', return_sequences=True, name='lstm1_encoder')(encoder_input)\n",
    "    encoder_output, h, c = tfkl.LSTM(128, activation='relu', return_state=True, name='lstm2_encoder')(lstm1)    \n",
    "    encoder_state = [h, c] \n",
    "    \n",
    "    # note that both h and c are passed to the decoder to initialize the state\n",
    "    # this is why we needed return_state = True\n",
    "    \n",
    "    #DECODER\n",
    "    #-------\n",
    "    decoder_input = tf.keras.Input(shape = output_shape, name='decoder_input')\n",
    "    # initialize the decoder state with the final encoder state\n",
    "    decoder_lstm, _, _ = tfkl.LSTM(units = 128, return_sequences = True, return_state = True, name ='lstm_dec')(decoder_input, initial_state = encoder_state)\n",
    "    decoder_out = tf.keras.layers.Dense(\n",
    "        embedded_dims, \n",
    "        activation='relu',\n",
    "        name = 'decoder_out'\n",
    "    )(decoder_lstm)\n",
    "    \n",
    "    #MODEL \n",
    "    model = tf.keras.Model([encoder_input, decoder_input], decoder_out)\n",
    "    \n",
    "    #---------------\n",
    "    # compile training model\n",
    "    model.compile(\n",
    "        loss=tfk.losses.MeanSquaredError(), \n",
    "        # so we don't have to use 1-hot encoding\n",
    "        optimizer = tfk.optimizers.Adam(learning_rate = 1e-2),\n",
    "        # large learning rate, probably because the net is simple and \n",
    "        # it takes a long time to train 1 epoch\n",
    "        metrics = ['mae']\n",
    "    )\n",
    "    \n",
    "    return model, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d6b88dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm1_encoder will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm2_encoder will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 32)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm1_encoder (LSTM)           (None, 15, 128)      82432       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 4, 32)]      0           []                               \n",
      "                                                                                                  \n",
      " lstm2_encoder (LSTM)           [(None, 128),        131584      ['lstm1_encoder[0][0]']          \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_dec (LSTM)                [(None, 4, 128),     82432       ['decoder_input[0][0]',          \n",
      "                                 (None, 128),                     'lstm2_encoder[0][1]',          \n",
      "                                 (None, 128)]                     'lstm2_encoder[0][2]']          \n",
      "                                                                                                  \n",
      " decoder_out (Dense)            (None, 4, 32)        4128        ['lstm_dec[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 300,576\n",
      "Trainable params: 300,576\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model, encoder_state = sequence2sequence(\n",
    "    embedded_dims=32, input_shape=input_shape, telescope = 4\n",
    ")\n",
    "\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a1f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the first input is set to zero\n",
    "encoder_input = X_emb\n",
    "padding = np.zeros([y_emb.shape[0], 1, y_emb.shape[2]])\n",
    "decoder_input = np.append( padding, y_emb,axis=1 )[:, :-1, :]\n",
    "target = y_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ffaced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6142, 4, 32), (6142, 4, 32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.shape, target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2e52b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11/11 [==============================] - 3s 117ms/step - loss: 0.0312 - mae: 0.1396 - val_loss: 0.0256 - val_mae: 0.1208 - lr: 0.0100\n",
      "Epoch 2/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0242 - mae: 0.1133 - val_loss: 0.0238 - val_mae: 0.1092 - lr: 0.0100\n",
      "Epoch 3/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0235 - mae: 0.1075 - val_loss: 0.0236 - val_mae: 0.1066 - lr: 0.0100\n",
      "Epoch 4/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0234 - mae: 0.1057 - val_loss: 0.0236 - val_mae: 0.1054 - lr: 0.0100\n",
      "Epoch 5/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0233 - mae: 0.1051 - val_loss: 0.0236 - val_mae: 0.1052 - lr: 0.0100\n",
      "Epoch 6/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0233 - mae: 0.1047 - val_loss: 0.0235 - val_mae: 0.1049 - lr: 0.0100\n",
      "Epoch 7/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0233 - mae: 0.1045 - val_loss: 0.0235 - val_mae: 0.1047 - lr: 0.0100\n",
      "Epoch 8/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0233 - mae: 0.1043 - val_loss: 0.0235 - val_mae: 0.1046 - lr: 0.0100\n",
      "Epoch 9/1000\n",
      "11/11 [==============================] - 1s 81ms/step - loss: 0.0233 - mae: 0.1042 - val_loss: 0.0235 - val_mae: 0.1045 - lr: 0.0100\n",
      "Epoch 10/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0233 - mae: 0.1040 - val_loss: 0.0235 - val_mae: 0.1043 - lr: 0.0100\n",
      "Epoch 11/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0233 - mae: 0.1038 - val_loss: 0.0235 - val_mae: 0.1042 - lr: 0.0100\n",
      "Epoch 12/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0233 - mae: 0.1036 - val_loss: 0.0235 - val_mae: 0.1040 - lr: 0.0100\n",
      "Epoch 13/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1034 - val_loss: 0.0235 - val_mae: 0.1038 - lr: 0.0100\n",
      "Epoch 14/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1033 - val_loss: 0.0235 - val_mae: 0.1037 - lr: 0.0100\n",
      "Epoch 15/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1032 - val_loss: 0.0235 - val_mae: 0.1037 - lr: 0.0100\n",
      "Epoch 16/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1031 - val_loss: 0.0235 - val_mae: 0.1036 - lr: 0.0100\n",
      "Epoch 17/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1031 - val_loss: 0.0235 - val_mae: 0.1036 - lr: 0.0100\n",
      "Epoch 18/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1031 - val_loss: 0.0235 - val_mae: 0.1036 - lr: 0.0100\n",
      "Epoch 19/1000\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.0232 - mae: 0.1030 - val_loss: 0.0235 - val_mae: 0.1036 - lr: 0.0100\n",
      "Epoch 20/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1030 - val_loss: 0.0235 - val_mae: 0.1035 - lr: 0.0100\n",
      "Epoch 21/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1030 - val_loss: 0.0235 - val_mae: 0.1035 - lr: 0.0100\n",
      "Epoch 22/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1029 - val_loss: 0.0235 - val_mae: 0.1035 - lr: 0.0100\n",
      "Epoch 23/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0232 - mae: 0.1029 - val_loss: 0.0235 - val_mae: 0.1034 - lr: 0.0100\n",
      "Epoch 24/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1029 - val_loss: 0.0235 - val_mae: 0.1034 - lr: 0.0100\n",
      "Epoch 25/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1029 - val_loss: 0.0235 - val_mae: 0.1033 - lr: 0.0100\n",
      "Epoch 26/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1028 - val_loss: 0.0235 - val_mae: 0.1033 - lr: 0.0100\n",
      "Epoch 27/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1028 - val_loss: 0.0235 - val_mae: 0.1033 - lr: 0.0100\n",
      "Epoch 28/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1028 - val_loss: 0.0235 - val_mae: 0.1033 - lr: 0.0100\n",
      "Epoch 29/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1027 - val_loss: 0.0235 - val_mae: 0.1032 - lr: 0.0100\n",
      "Epoch 30/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1027 - val_loss: 0.0235 - val_mae: 0.1032 - lr: 0.0100\n",
      "Epoch 31/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1027 - val_loss: 0.0235 - val_mae: 0.1032 - lr: 0.0100\n",
      "Epoch 32/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1027 - val_loss: 0.0235 - val_mae: 0.1032 - lr: 0.0100\n",
      "Epoch 33/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1026 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 34/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1026 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 35/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0232 - mae: 0.1026 - val_loss: 0.0235 - val_mae: 0.1032 - lr: 0.0100\n",
      "Epoch 36/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1026 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 37/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1026 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 38/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 39/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1030 - lr: 0.0100\n",
      "Epoch 40/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 41/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1026 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 42/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 43/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1030 - lr: 0.0100\n",
      "Epoch 44/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1030 - lr: 0.0100\n",
      "Epoch 45/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1030 - lr: 0.0100\n",
      "Epoch 46/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1024 - val_loss: 0.0235 - val_mae: 0.1030 - lr: 0.0100\n",
      "Epoch 47/1000\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1031 - lr: 0.0100\n",
      "Epoch 48/1000\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.0232 - mae: 0.1026 - val_loss: 0.0235 - val_mae: 0.1030 - lr: 0.0100\n",
      "Epoch 49/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1025 - val_loss: 0.0235 - val_mae: 0.1029 - lr: 0.0100\n",
      "Epoch 50/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1024 - val_loss: 0.0235 - val_mae: 0.1029 - lr: 0.0100\n",
      "Epoch 51/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1024 - val_loss: 0.0235 - val_mae: 0.1029 - lr: 0.0100\n",
      "Epoch 52/1000\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.0232 - mae: 0.1024 - val_loss: 0.0234 - val_mae: 0.1029 - lr: 0.0100\n",
      "Epoch 53/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1024 - val_loss: 0.0235 - val_mae: 0.1029 - lr: 0.0100\n",
      "Epoch 54/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1024 - val_loss: 0.0234 - val_mae: 0.1028 - lr: 0.0100\n",
      "Epoch 55/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0232 - mae: 0.1024 - val_loss: 0.0234 - val_mae: 0.1028 - lr: 0.0100\n",
      "Epoch 56/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0232 - mae: 0.1023 - val_loss: 0.0234 - val_mae: 0.1028 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0232 - mae: 0.1023 - val_loss: 0.0234 - val_mae: 0.1029 - lr: 0.0100\n",
      "Epoch 58/1000\n",
      "11/11 [==============================] - 1s 81ms/step - loss: 0.0232 - mae: 0.1023 - val_loss: 0.0234 - val_mae: 0.1028 - lr: 0.0100\n",
      "Epoch 59/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0232 - mae: 0.1023 - val_loss: 0.0234 - val_mae: 0.1028 - lr: 0.0100\n",
      "Epoch 60/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0231 - mae: 0.1021 - val_loss: 0.0238 - val_mae: 0.1079 - lr: 0.0100\n",
      "Epoch 61/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0225 - mae: 0.1030 - val_loss: 0.0225 - val_mae: 0.0998 - lr: 0.0100\n",
      "Epoch 62/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0222 - mae: 0.0991 - val_loss: 0.0224 - val_mae: 0.0984 - lr: 0.0100\n",
      "Epoch 63/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0980 - val_loss: 0.0224 - val_mae: 0.0981 - lr: 0.0100\n",
      "Epoch 64/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0974 - val_loss: 0.0224 - val_mae: 0.0978 - lr: 0.0100\n",
      "Epoch 65/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0971 - val_loss: 0.0224 - val_mae: 0.0976 - lr: 0.0100\n",
      "Epoch 66/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0970 - val_loss: 0.0224 - val_mae: 0.0975 - lr: 0.0100\n",
      "Epoch 67/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0969 - val_loss: 0.0224 - val_mae: 0.0975 - lr: 0.0100\n",
      "Epoch 68/1000\n",
      "11/11 [==============================] - 1s 82ms/step - loss: 0.0221 - mae: 0.0969 - val_loss: 0.0224 - val_mae: 0.0974 - lr: 0.0100\n",
      "Epoch 69/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0969 - val_loss: 0.0224 - val_mae: 0.0974 - lr: 0.0100\n",
      "Epoch 70/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0968 - val_loss: 0.0224 - val_mae: 0.0974 - lr: 0.0100\n",
      "Epoch 71/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0968 - val_loss: 0.0224 - val_mae: 0.0974 - lr: 0.0100\n",
      "Epoch 72/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0968 - val_loss: 0.0224 - val_mae: 0.0974 - lr: 0.0100\n",
      "Epoch 73/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0221 - mae: 0.0968 - val_loss: 0.0224 - val_mae: 0.0974 - lr: 0.0100\n",
      "Epoch 74/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0968 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 75/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0967 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 76/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0967 - val_loss: 0.0224 - val_mae: 0.0974 - lr: 0.0100\n",
      "Epoch 77/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0221 - mae: 0.0967 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 78/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0967 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 79/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0967 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 80/1000\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.0221 - mae: 0.0967 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 81/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 82/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 83/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 84/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 85/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 86/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 87/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0973 - lr: 0.0100\n",
      "Epoch 88/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 89/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0221 - mae: 0.0966 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 90/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0221 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 91/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0221 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 92/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0221 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 93/1000\n",
      "11/11 [==============================] - 1s 81ms/step - loss: 0.0221 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 94/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0221 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 95/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 96/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 97/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0221 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 98/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0221 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 99/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 100/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0965 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 101/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 102/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 103/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 104/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 105/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 106/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 107/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 108/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 109/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 110/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 111/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 112/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 114/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 115/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 116/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 117/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0224 - val_mae: 0.0972 - lr: 0.0100\n",
      "Epoch 118/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0964 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 119/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 120/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 121/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 122/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 123/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 124/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 125/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 126/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 127/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 128/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 129/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 130/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 131/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 132/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 133/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 134/1000\n",
      "11/11 [==============================] - 1s 82ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 135/1000\n",
      "11/11 [==============================] - 1s 81ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 136/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0971 - lr: 0.0100\n",
      "Epoch 137/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 138/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 139/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 140/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 141/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 142/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0963 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 143/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 144/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 145/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 146/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 147/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 148/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 149/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 150/1000\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 151/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 152/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 153/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 154/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 155/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 156/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 157/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 158/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 159/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 160/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 161/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 162/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 163/1000\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 164/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0970 - lr: 0.0100\n",
      "Epoch 165/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 166/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 167/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 169/1000\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.0220 - mae: 0.0961 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 170/1000\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.0220 - mae: 0.0961 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 171/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0961 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 172/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0961 - val_loss: 0.0223 - val_mae: 0.0968 - lr: 0.0100\n",
      "Epoch 173/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0961 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 174/1000\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n",
      "Epoch 175/1000\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.0220 - mae: 0.0962 - val_loss: 0.0223 - val_mae: 0.0969 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "#train the RNN\n",
    "history = seq2seq_model.fit(\n",
    "    x=[encoder_input, decoder_input],\n",
    "    y=target,\n",
    "    batch_size = batch_size,\n",
    "    epochs=1000,\n",
    "    validation_split=.1,\n",
    "    callbacks=[\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "            ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=5,\n",
    "            factor=0.5,\n",
    "            min_lr=1e-2\n",
    "            ),\n",
    "        ]\n",
    "    ).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d4e147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveSession.close(self=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdd86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.7",
   "language": "python",
   "name": "tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "531.844px",
    "left": "1541px",
    "right": "20px",
    "top": "120px",
    "width": "359px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
